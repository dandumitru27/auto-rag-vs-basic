{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a3f214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\auto-rag-vs-basic\\.venv\\lib\\site-packages\\google\\api_core\\_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.11) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lightrag.llm.gemini import gemini_complete_if_cache, gemini_embed\n",
    "from lightrag.llm.openai import openai_complete_if_cache, openai_embed\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "from raganything import RAGAnything, RAGAnythingConfig\n",
    "\n",
    "provider = \"openai\"  # \"google_genai\" / \"openai\"\n",
    "\n",
    "if provider == \"google_genai\":\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "elif provider == \"openai\":\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d11cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RAGAnythingConfig(\n",
    "    working_dir=\"../rag_storage\",\n",
    "    parser=\"docling\",  # document parser (mineru or docling)\n",
    "    parse_method=\"txt\",  # auto/ocr/txt\n",
    "    enable_image_processing=False,\n",
    "    enable_table_processing=False,\n",
    "    enable_equation_processing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159aaaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model_func(prompt, system_prompt=None, history_messages=None, **kwargs):\n",
    "    if history_messages is None:\n",
    "        history_messages = []\n",
    "\n",
    "    if provider == \"google_genai\":\n",
    "        return gemini_complete_if_cache(\n",
    "            \"gemini-2.5-flash\",\n",
    "            prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            history_messages=history_messages,\n",
    "            api_key=api_key,\n",
    "            **kwargs,\n",
    "        )\n",
    "    elif provider == \"openai\":\n",
    "        return openai_complete_if_cache(\n",
    "            \"gpt-5-mini\",  # model name\n",
    "            prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            history_messages=history_messages,\n",
    "            api_key=api_key,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d7b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "if provider == \"google_genai\":\n",
    "    async def gemini_embedding(texts):\n",
    "        return await gemini_embed(\n",
    "            texts,\n",
    "            model=\"models/gemini-embedding-001\",\n",
    "            api_key=api_key,\n",
    "        )\n",
    "\n",
    "    embedding_func = EmbeddingFunc(\n",
    "        embedding_dim=768,\n",
    "        max_token_size=8192,\n",
    "        func=gemini_embedding,\n",
    "    )\n",
    "elif provider == \"openai\":\n",
    "    client = AsyncOpenAI(api_key=api_key)\n",
    "\n",
    "    async def openai_embedding(texts):\n",
    "        single_input = isinstance(texts, str)\n",
    "        if single_input:\n",
    "            texts = [texts]\n",
    "\n",
    "        resp = await client.embeddings.create(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            input=texts,\n",
    "        )\n",
    "        \n",
    "        data_sorted = sorted(resp.data, key=lambda r: r.index)\n",
    "        matrix = np.asarray([row.embedding for row in data_sorted], dtype=np.float32)\n",
    "\n",
    "        # Hard guard: one vector per input text\n",
    "        if matrix.shape[0] != len(texts):\n",
    "            raise ValueError(\n",
    "                f\"Vector count mismatch: expected {len(texts)} vectors but got {matrix.shape[0]} vectors.\"\n",
    "            )\n",
    "\n",
    "        # Return 1D for single text, 2D for batch\n",
    "        return matrix[0] if single_input else matrix\n",
    "\n",
    "    embedding_func = EmbeddingFunc(\n",
    "        embedding_dim=3072,\n",
    "        max_token_size=8192,\n",
    "        func=openai_embedding,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a97192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: RAGAnything initialized with config:\n",
      "INFO:   Working directory: ../rag_storage\n",
      "INFO:   Parser: docling\n",
      "INFO:   Parse method: txt\n",
      "INFO:   Multimodal processing - Image: False, Table: False, Equation: False\n",
      "INFO:   Max concurrent files: 1\n"
     ]
    }
   ],
   "source": [
    "rag = RAGAnything(\n",
    "    config=config,\n",
    "    llm_model_func=llm_model_func,\n",
    "    embedding_func=embedding_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbd3eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Parser 'docling' installation verified\n",
      "INFO: Initializing LightRAG with parameters: {'working_dir': '../rag_storage'}\n",
      "INFO: [] Created new empty graph file: ../rag_storage\\graph_chunk_entity_relation.graphml\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': '../rag_storage\\\\vdb_entities.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': '../rag_storage\\\\vdb_relationships.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': '../rag_storage\\\\vdb_chunks.json'} 0 data\n",
      "INFO: Multimodal processors initialized with context support\n",
      "INFO: Available processors: ['generic']\n",
      "INFO: Context configuration: ContextConfig(context_window=1, context_mode='page', max_context_tokens=2000, include_headers=True, include_captions=True, filter_content_types=['text'])\n",
      "INFO: LightRAG, parse cache, and multimodal processors initialized\n",
      "INFO: Starting complete document processing: ..\\pdf\\returul_unui_produs.pdf\n",
      "INFO: Starting document parsing: ..\\pdf\\returul_unui_produs.pdf\n",
      "INFO: Using cached parsing result for: ..\\pdf\\returul_unui_produs.pdf\n",
      "INFO: * Total blocks in cached content_list: 21\n",
      "INFO: Content separation complete:\n",
      "INFO:   - Text content length: 2874 characters\n",
      "INFO:   - Multimodal items count: 0\n",
      "INFO: Starting text content insertion into LightRAG...\n",
      "WARNING: Ignoring document ID (already exists): doc-ad4cb0a9477245d38c83e5d8e360b6c8 (returul_unui_produs.pdf)\n",
      "WARNING: No new unique documents were found.\n",
      "INFO: Reset 1 documents from PROCESSING/FAILED to PENDING status\n",
      "INFO: Processing 1 document(s)\n",
      "INFO: Extracting stage 1/1: returul_unui_produs.pdf\n",
      "INFO: Processing d-id: doc-ad4cb0a9477245d38c83e5d8e360b6c8\n",
      "INFO: Embedding func: 8 new workers initialized (Timeouts: Func: 30s, Worker: 60s, Health Check: 75s)\n",
      "INFO: LLM func: 4 new workers initialized (Timeouts: Func: 180s, Worker: 360s, Health Check: 375s)\n",
      "INFO:  == LLM cache == saving: default:extract:e6dbc6934a6953a70af94274bfc23a78\n",
      "INFO:  == LLM cache == saving: default:extract:43761f54d4000df7f6b7354c3f68c28c\n",
      "INFO: Chunk 1 of 1 extracted 23 Ent + 25 Rel chunk-cdc00b93663ecc9e88851008dd5421bc\n",
      "INFO: Merging stage 1/1: returul_unui_produs.pdf\n",
      "INFO: Phase 1: Processing 23 entities from doc-ad4cb0a9477245d38c83e5d8e360b6c8 (async: 8)\n",
      "INFO: Phase 2: Processing 25 relations from doc-ad4cb0a9477245d38c83e5d8e360b6c8 (async: 8)\n",
      "INFO: Phase 3: Updating final 23(23+0) entities and  25 relations from doc-ad4cb0a9477245d38c83e5d8e360b6c8\n",
      "INFO: Completed merging: 23 entities, 0 extra entities, 25 relations\n",
      "INFO: [] Writing graph with 23 nodes, 25 edges\n",
      "INFO: In memory DB persist to disk\n",
      "INFO: Completed processing file 1/1: returul_unui_produs.pdf\n",
      "INFO: Enqueued document processing pipeline stopped\n",
      "INFO: Text content insertion complete\n",
      "INFO: Document ..\\pdf\\returul_unui_produs.pdf processing complete!\n"
     ]
    }
   ],
   "source": [
    "# file_path = \"C:\\\\Dev\\\\EPO_Patent_PDFs\\\\EP11869524NWA1.pdf\"\n",
    "file_path = \"..\\\\pdf\\\\returul_unui_produs.pdf\"\n",
    "\n",
    "await rag.process_document_complete(file_path, output_dir=\"../output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f710bb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing text query: Cum vor fi rambursati banii?...\n",
      "INFO: Query mode: mix\n",
      "INFO:  == LLM cache == saving: mix:keywords:0bf408b51c70a374a32852272f2c2b72\n",
      "INFO: Query nodes: Refund method, Bank transfer, Credit card refund, Processing time, Transaction ID, Refund amount (top_k:40, cosine:0.2)\n",
      "INFO: Local query: 21 entites, 25 relations\n",
      "INFO: Query edges: Refund process, Money reimbursement, Refund policy, Refund timeline (top_k:40, cosine:0.2)\n",
      "INFO: Global query: 23 entites, 25 relations\n",
      "INFO: Naive query: 1 chunks (chunk_top_k:20 cosine:0.2)\n",
      "INFO: Raw search results: 23 entities, 25 relations, 1 vector chunks\n",
      "INFO: After truncation: 23 entities, 25 relations\n",
      "INFO: Selecting 1 from 1 entity-related chunks by vector similarity\n",
      "INFO: Find no additional relations-related chunks from 25 relations\n",
      "INFO: Round-robin merged chunks: 2 -> 1 (deduplicated 1)\n",
      "WARNING: Rerank is enabled but no rerank model is configured. Please set up a rerank model or set enable_rerank=False in query parameters.\n",
      "INFO: Final context: 23 entities, 25 relations, 1 chunks\n",
      "INFO: Final chunks S+F/O: E23/1\n",
      "INFO:  == LLM cache == saving: mix:query:160c0088bcb12e8e20f5c58697574609\n",
      "INFO: Text query completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Rambursarea banilor\n",
      "\n",
      "- **Metoda de rambursare:** Suma va fi restituită, în mod normal, prin aceeași metodă de plată folosită la plasarea comenzii. Dacă ai solicitat altfel, rambursarea se poate face conform cererii tale.  \n",
      "- **Plăți online cu cardul:** În cazul plăților efectuate online cu cardul, suma este returnată direct în contul bancar asociat cardului utilizat.  \n",
      "- **Termen:** Contravaloarea produsului returnat va fi rambursată în termen de **14 zile calendaristice** de la confirmarea aprobării returului.  \n",
      "- **Confirmarea aprobării returului:** Rambursarea este inițiată după ce returul este aprobat (după verificarea produselor de către echipă, conform procedurii de retur).\n",
      "\n",
      "### References\n",
      "\n",
      "- [1] returul_unui_produs.pdf\n"
     ]
    }
   ],
   "source": [
    "answer = await rag.aquery(\"Cum vor fi rambursati banii?\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
